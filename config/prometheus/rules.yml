---
# Prometheus Alert Rules - Production N8N Stack
# Comprehensive alerting for service health and performance

groups:
  # Docker Container Resource Alerts
  - name: container_resources
    interval: 30s
    rules:
      # High container CPU usage
      - alert: HighContainerCPU
        expr: |
          rate(engine_daemon_container_cpu_usage_seconds_total{
            container_name=~"n8n.*|postgres.*|redis.*|traefik.*|grafana.*"
          }[5m]) > 0.80
        for: 5m
        annotations:
          summary: "High CPU usage in container {{ $labels.container_name }}"
          description: "Container {{ $labels.container_name }} CPU usage is {{ $value | humanizePercentage }} for 5 minutes"
          severity: warning
        labels:
          service: docker
          severity: warning
          container: "{{ $labels.container_name }}"

      # Critical container CPU usage
      - alert: CriticalContainerCPU
        expr: |
          rate(engine_daemon_container_cpu_usage_seconds_total{
            container_name=~"n8n.*|postgres.*|redis.*|traefik.*"
          }[5m]) > 0.95
        for: 2m
        annotations:
          summary: "Critical CPU usage in container {{ $labels.container_name }}"
          description: "Container {{ $labels.container_name }} CPU usage is {{ $value | humanizePercentage }} - immediate action required"
          severity: critical
        labels:
          service: docker
          severity: critical
          container: "{{ $labels.container_name }}"

      # High container memory usage
      - alert: HighContainerMemory
        expr: |
          (engine_daemon_container_memory_usage_bytes{
            container_name=~"n8n.*|postgres.*|redis.*|traefik.*|grafana.*"
          } / 
          engine_daemon_container_memory_limit_bytes) > 0.85
        for: 5m
        annotations:
          summary: "High memory usage in container {{ $labels.container_name }}"
          description: "Container {{ $labels.container_name }} memory usage is {{ $value | humanizePercentage }}"
          severity: warning
        labels:
          service: docker
          severity: warning
          container: "{{ $labels.container_name }}"

      # Critical container memory usage
      - alert: CriticalContainerMemory
        expr: |
          (engine_daemon_container_memory_usage_bytes{
            container_name=~"n8n.*|postgres.*|redis.*|traefik.*"
          } / 
          engine_daemon_container_memory_limit_bytes) > 0.95
        for: 2m
        annotations:
          summary: "Critical memory usage in container {{ $labels.container_name }}"
          description: "Container {{ $labels.container_name }} memory at {{ $value | humanizePercentage }} - OOM risk"
          severity: critical
        labels:
          service: docker
          severity: critical
          container: "{{ $labels.container_name }}"

      # Container health check failing
      - alert: ContainerUnhealthy
        expr: |
          engine_daemon_health_checks_failed_total{
            container_name=~"n8n.*|postgres.*|redis.*|traefik.*"
          } > 3
        for: 2m
        annotations:
          summary: "Container {{ $labels.container_name }} health check failing"
          description: "Container has failed {{ $value }} consecutive health checks"
          severity: critical
        labels:
          service: docker
          severity: critical
          container: "{{ $labels.container_name }}"

  # Traefik Ingress Alerts
  - name: traefik_ingress
    interval: 30s
    rules:
      # High request error rate
      - alert: HighTraefikErrorRate
        expr: |
          (sum(rate(traefik_service_requests_total{code=~"5.."}[5m])) by (service) /
           sum(rate(traefik_service_requests_total[5m])) by (service)) > 0.05
        for: 5m
        annotations:
          summary: "High error rate on Traefik service {{ $labels.service }}"
          description: "{{ $value | humanizePercentage }} of requests returning 5xx errors"
          severity: warning
        labels:
          service: traefik
          severity: warning

      # Critical error rate
      - alert: CriticalTraefikErrorRate
        expr: |
          (sum(rate(traefik_service_requests_total{code=~"5.."}[5m])) by (service) /
           sum(rate(traefik_service_requests_total[5m])) by (service)) > 0.20
        for: 2m
        annotations:
          summary: "Critical error rate on Traefik service {{ $labels.service }}"
          description: "{{ $value | humanizePercentage }} of requests failing - service degraded"
          severity: critical
        labels:
          service: traefik
          severity: critical

      # High request latency
      - alert: HighTraefikLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(traefik_service_request_duration_seconds_bucket[5m])) by (service, le)
          ) > 5
        for: 10m
        annotations:
          summary: "High latency on Traefik service {{ $labels.service }}"
          description: "P95 latency is {{ $value }}s (threshold: 5s)"
          severity: warning
        labels:
          service: traefik
          severity: warning

      # Backend service down
      - alert: TraefikBackendDown
        expr: traefik_service_server_up == 0
        for: 1m
        annotations:
          summary: "Traefik backend {{ $labels.service }} is down"
          description: "Backend service {{ $labels.service }} is unreachable"
          severity: critical
        labels:
          service: traefik
          severity: critical

  # Service Health Alerts
  - name: service_health
    interval: 30s
    rules:
      # Prometheus down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 2m
        annotations:
          summary: "Prometheus monitoring is down"
          description: "Prometheus has been unreachable for 2+ minutes - monitoring blind"
          severity: critical
        labels:
          service: prometheus
          severity: critical

      # Traefik down
      - alert: TraefikDown
        expr: up{job="traefik"} == 0
        for: 1m
        annotations:
          summary: "Traefik reverse proxy is down"
          description: "All ingress traffic blocked - immediate intervention required"
          severity: critical
        labels:
          service: traefik
          severity: critical

      # AlertManager down
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        annotations:
          summary: "AlertManager is down"
          description: "Alert notifications will not be delivered"
          severity: warning
        labels:
          service: alertmanager
          severity: warning

      # Docker daemon unreachable
      - alert: DockerDaemonDown
        expr: up{job="docker"} == 0
        for: 1m
        annotations:
          summary: "Docker daemon metrics unavailable"
          description: "Cannot collect container metrics - check Docker daemon"
          severity: warning
        labels:
          service: docker
          severity: warning

      # Grafana down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 5m
        annotations:
          summary: "Grafana dashboard is down"
          description: "Visualization interface unavailable"
          severity: warning
        labels:
          service: grafana
          severity: warning

  # Docker System Health
  - name: docker_system
    interval: 30s
    rules:
      # High number of containers
      - alert: TooManyContainers
        expr: engine_daemon_container_states_containers{state="running"} > 50
        for: 10m
        annotations:
          summary: "High number of running containers"
          description: "{{ $value }} containers running - potential resource exhaustion"
          severity: warning
        labels:
          service: docker
          severity: warning

      # Container restarts
      - alert: ContainerRestarting
        expr: |
          rate(engine_daemon_container_actions_seconds_count{action="restart"}[5m]) > 0.1
        for: 5m
        annotations:
          summary: "Containers restarting frequently"
          description: "Restart rate: {{ $value }} restarts/sec"
          severity: warning
        labels:
          service: docker
          severity: warning

  # AlertManager Health
  - name: alertmanager_health
    interval: 60s
    rules:
      # Notifications failing
      - alert: AlertmanagerNotificationsFailing
        expr: |
          rate(alertmanager_notifications_failed_total[5m]) > 0.1
        for: 5m
        annotations:
          summary: "AlertManager notification failures"
          description: "{{ $value }} notifications/sec failing to send"
          severity: warning
        labels:
          service: alertmanager
          severity: warning

      # High alert load
      - alert: HighAlertLoad
        expr: alertmanager_alerts > 100
        for: 10m
        annotations:
          summary: "High number of active alerts"
          description: "{{ $value }} active alerts - potential alert storm"
          severity: warning
        labels:
          service: alertmanager
          severity: warning
